# Simple Agent Configuration

# LLM Provider Settings
llm:
  # Active provider: openai, ollama, lmstudio, anthropic
  provider: "openai"

  # OpenAI Configuration
  openai:
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"  # References .env
    temperature: 0.7
    max_tokens: 2000

  # LM Studio Configuration (local, OpenAI-compatible API)
  lmstudio:
    model: "llama-3.2-1b-instruct"
    base_url: "http://localhost:1234/v1"
    temperature: 0.7
    max_tokens: 2000

  # Ollama Configuration (local)
  ollama:
    model: "llama3.2:1b"
    base_url: "http://localhost:11434"
    temperature: 0.7
    max_tokens: 2000

  # Anthropic Configuration
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    api_key: "${ANTHROPIC_API_KEY}"  # References .env
    temperature: 0.7
    max_tokens: 2000

# Agent Settings
agents:
  # Default agent configuration
  default:
    role: |
      You are a helpful AI assistant with access to tools.

      When answering questions:
      1. If you need to use a tool, call it ONCE with the appropriate arguments
      2. After getting the tool result, immediately call final_answer with your response
      3. Never call the same tool multiple times for the same question
      4. Be concise and direct

      Always use the final_answer tool to provide your final response.
    verbosity: 1  # 0=quiet, 1=normal, 2=verbose
    max_steps: 10  # Max tool call iterations
    agent_type: "tool_calling"  # "tool_calling" (safe, default) or "code" (requires Docker)
    # executor_type: "docker"  # Only for agent_type="code": "docker", "e2b", "modal", "wasm"

# Debug Settings
debug:
  level: "info"  # off (minimal), info (normal), debug (verbose)
  # Future: display_mode (full/minimal), output_template

# Logging
logging:
  level: "INFO"
  file: "logs/app.log"
  console_enabled: false
